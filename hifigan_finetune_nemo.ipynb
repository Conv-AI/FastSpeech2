{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0044fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-11-26 03:26:01 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-26 03:26:02 experimental:27] Module <class 'nemo.collections.tts.models.fastpitch_ssl.FastPitchModel_SSL'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-26 03:26:02 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-26 03:26:02 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-26 03:26:02 experimental:27] Module <class 'nemo.collections.tts.models.ssl_tts.SSLDisentangler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-26 03:26:02 experimental:27] Module <class 'nemo.collections.tts.models.vits.VitsModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from model import FastSpeech2\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02476e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "model_config = yaml.load(open('/workspace/nemo/vol/FastSpeech2/config/Nova/model.yaml',\n",
    "                              \"r\"), Loader=yaml.FullLoader)\n",
    "preprocess_config = yaml.load(open('/workspace/nemo/vol/FastSpeech2/config/Nova/preprocess.yaml',\n",
    "                              \"r\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d36f13c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FastSpeech2(model_config=model_config, preprocess_config=preprocess_config)\n",
    "ckpt_file_path = '/workspace/nemo/vol/FastSpeech2/output/ckpt/Nova/50000.pth.tar'\n",
    "checkpoint = torch.load(ckpt_file_path, map_location=torch.device('cuda'))\n",
    "fp.load_state_dict(checkpoint['model'])\n",
    "fp = fp.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c84a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# fp = nn.DataParallel(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081485c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint['model']['emotion_emb.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10ea397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def load_wav(audio_file, target_sr=None):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "        sample_rate = f.samplerate\n",
    "        if target_sr is not None and target_sr != sample_rate:\n",
    "            samples = librosa.core.resample(samples, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    return samples.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68b60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('/workspace/nemo/vol/LBM_EB/preprocessed_data/LBM_EB/paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5eed633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rec = []\n",
    "\n",
    "# speakers_path = \"/workspace/nemo/vol/LBM_EB/preprocessed_data/LBM_EB/speakers.json\"\n",
    "# emotions_path = \"/workspace/nemo/vol/LBM_EB/preprocessed_data/LBM_EB/emotions.json\"\n",
    "\n",
    "# with open(speakers_path, \"r\") as f:\n",
    "#     speakers = json.load(f)\n",
    "    \n",
    "# with open(emotions_path, \"r\") as f:\n",
    "#     emotions = json.load(f)\n",
    "\n",
    "# for i, row in tqdm(df[886248:].iterrows()):\n",
    "#     text_path = row.text_paths\n",
    "#     wav_path = row.wav_paths\n",
    "#     try:\n",
    "#         with open(text_path, 'r') as f:\n",
    "#             text = f.read()\n",
    "#         duration = librosa.get_duration(filename=wav_path)\n",
    "#         speaker = text_path.split('/')[-3]\n",
    "#         emotion = text_path.split('/')[-2]\n",
    "#         speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "#         emotion_ = [v for k,v in emotions.items() if k==emotion][0] \n",
    "\n",
    "#         r = {\n",
    "#            \"audio_filepath\" : wav_path,\n",
    "#            \"text\" : text,\n",
    "#            \"speaker\": speaker_,\n",
    "#            \"emotion\": emotion_,\n",
    "#            \"duration\" : round(duration, 1),\n",
    "#            \"text_no_preprocessing\" : text\n",
    "#         }\n",
    "#         rec.append(r)\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2eea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b558e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Sample array\n",
    "# #data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# # Serialize the array to a JSON-formatted string\n",
    "# json_data = json.dumps(rec)\n",
    "\n",
    "# # Write the JSON string to a file\n",
    "# with open('data_rec.json', 'w') as file:\n",
    "#     file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9dfaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('data_rec.json', 'r') as file:\n",
    "#     rec = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "027f724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5108fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d4898e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14817it [00:04, 2973.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# file_paths = list()\n",
    "rec = []\n",
    "\n",
    "voices_path = '/workspace/nemo/vol/extvol3/NovaEmo/RawData/'\n",
    "base_path = voices_path\n",
    "\n",
    "preprocess_dir = \"/workspace/nemo/vol/extvol3/NovaEmo/preprocessed_data/\"\n",
    "speakers_path = \"/workspace/nemo/vol/extvol3/NovaEmo/preprocessed_data/speakers.json\"\n",
    "emotions_path = \"/workspace/nemo/vol/extvol3/NovaEmo/preprocessed_data/emotions.json\"\n",
    "\n",
    "with open(preprocess_dir+'train.txt', 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        file, speaker, emotion, ph, text = line.split('|')\n",
    "        if not speaker == 'NovaConvai':\n",
    "            continue\n",
    "        audio_filepath = os.path.join(base_path, speaker, emotion, '.'.join([file, 'wav']))\n",
    "        text_filepath = os.path.join(base_path, speaker, emotion, '.'.join([file, 'lab']))\n",
    "#         file_paths.append((audio_filepath, text_filepath))\n",
    "        duration = librosa.get_duration(filename=audio_filepath)\n",
    "        with open(text_filepath, 'r') as f:\n",
    "            text = f.read()\n",
    "        with open(speakers_path, \"r\") as f:\n",
    "            speakers = json.load(f)\n",
    "            speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "        with open(emotions_path, \"r\") as f:\n",
    "            emotions = json.load(f)\n",
    "            emotion_ = [v for k,v in emotions.items() if k==emotion][0]   \n",
    "        r = {\n",
    "           \"audio_filepath\" : audio_filepath,\n",
    "           \"text\" : text,\n",
    "           \"speaker\": speaker_,\n",
    "           \"emotion\": emotion_,\n",
    "           \"duration\" : round(duration,1),\n",
    "           \"text_no_preprocessing\" : text\n",
    "        }\n",
    "        rec.append(r)\n",
    "        \n",
    "with open(preprocess_dir+'val.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        file, speaker, emotion, ph, text = line.split('|')\n",
    "        if not speaker == 'NovaConvai':\n",
    "            continue\n",
    "        audio_filepath = os.path.join(base_path, speaker, emotion, '.'.join([file, 'wav']))\n",
    "        text_filepath = os.path.join(base_path, speaker, emotion, '.'.join([file, 'lab']))\n",
    "#         file_paths.append((audio_filepath, text_filepath))\n",
    "        duration = librosa.get_duration(filename=audio_filepath)\n",
    "        with open(text_filepath, 'r') as f:\n",
    "            text = f.read()\n",
    "        with open(speakers_path, \"r\") as f:\n",
    "            speakers = json.load(f)\n",
    "            speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "        with open(emotions_path, \"r\") as f:\n",
    "            emotions = json.load(f)\n",
    "            emotion_ = [v for k,v in emotions.items() if k==emotion][0]   \n",
    "        r = {\n",
    "           \"audio_filepath\" : audio_filepath,\n",
    "           \"text\" : text,\n",
    "           \"speaker\": speaker_,\n",
    "           \"emotion\": emotion_,\n",
    "           \"duration\" : round(duration,1),\n",
    "           \"text_no_preprocessing\" : text\n",
    "        }\n",
    "        rec.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26ad1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voices_path = '/workspace/nemo/vol/extvol3/NovaEmo/RawData/'\n",
    "\n",
    "# base_path = voices_path\n",
    "\n",
    "# speakers_path = \"/workspace/nemo/vol/extvol3/NovaEmo/preprocessed_data/speakers.json\"\n",
    "# emotions_path = \"/workspace/nemo/vol/extvol3/NovaEmo/preprocessed_data/emotions.json\"\n",
    "\n",
    "# rec = []\n",
    "\n",
    "# # speaker = 'Female-V2'\n",
    "# covered= []\n",
    "\n",
    "# for speaker in os.listdir(base_path)\n",
    "\n",
    "#     for emotion in os.listdir(base_path):\n",
    "#         emotion_path = os.path.join(base_path, emotion)\n",
    "#         print(emotion_path)\n",
    "\n",
    "#         for file in os.listdir(emotion_path):\n",
    "#             try:\n",
    "#                 filename = file.split('.')[0]\n",
    "#                 if filename in covered:\n",
    "#                     continue\n",
    "#                 covered.append(filename)\n",
    "\n",
    "#                 audio_filepath = os.path.join(emotion_path, '.'.join([filename, 'wav'])) \n",
    "#                 text_filepath = os.path.join(emotion_path, '.'.join([filename, 'lab']))\n",
    "#                 #print(text_filepath)\n",
    "#                 duration = librosa.get_duration(filename=audio_filepath)\n",
    "#                 with open(text_filepath, 'r') as f:\n",
    "#                     text = f.read()\n",
    "#                 with open(speakers_path, \"r\") as f:\n",
    "#                     speakers = json.load(f)\n",
    "#                     speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "#                 with open(emotions_path, \"r\") as f:\n",
    "#                     emotions = json.load(f)\n",
    "#                     emotion_ = [v for k,v in emotions.items() if k==emotion][0]   \n",
    "#                 r = {\n",
    "#                    \"audio_filepath\" : audio_filepath,\n",
    "#                    \"text\" : text,\n",
    "#                    \"speaker\": speaker_,\n",
    "#                    \"emotion\": emotion_,\n",
    "#                    \"duration\" : round(duration,1),\n",
    "#                    \"text_no_preprocessing\" : text\n",
    "#                 }\n",
    "#                 rec.append(r)\n",
    "#             except:\n",
    "#                 continue\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bee0a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(rec)\n",
    "train_len = int(0.9*len(rec))\n",
    "train_rec = rec[:train_len]\n",
    "val_rec = rec[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5edab084",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = f'./fastpitch_train.json'\n",
    "with open(train_manifest, \"w\") as f:\n",
    "    for s in train_rec:\n",
    "        f.write(json.dumps(s) + '\\n')\n",
    "        \n",
    "val_manifest = f'./fastpitch_val.json'\n",
    "with open(val_manifest, \"w\") as f:\n",
    "    for s in val_rec:\n",
    "        f.write(json.dumps(s) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6e35900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_rec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dac64760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_filepath': '/workspace/nemo/vol/extvol3/NovaEmo/RawData/NovaConvai/NovaNeutral/0c710b31981437a90f6f02bfb9742e1053dfb61dc0d4b6d8f25c5005554d51eb_chunk_189.wav',\n",
       " 'text': \"That ain't all the Japanese talk.\",\n",
       " 'speaker': 163,\n",
       " 'emotion': 2,\n",
       " 'duration': 1.5,\n",
       " 'text_no_preprocessing': \"That ain't all the Japanese talk.\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13b3b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "from string import punctuation\n",
    "import re\n",
    "from text import text_to_sequence\n",
    "\n",
    "\n",
    "def read_lexicon(lex_path):\n",
    "    lexicon = {}\n",
    "    with open(lex_path) as f:\n",
    "        for line in f:\n",
    "            temp = re.split(r\"\\s+\", line.strip(\"\\n\"))\n",
    "            word = temp[0]\n",
    "            phones = temp[1:]\n",
    "            if word.lower() not in lexicon:\n",
    "                lexicon[word.lower()] = phones\n",
    "    return lexicon\n",
    "\n",
    "g2p = G2p()\n",
    "lexicon = read_lexicon(preprocess_config[\"path\"][\"lexicon_path\"])\n",
    "\n",
    "def preprocess_english(text, preprocess_config, g2p = g2p, lexicon = lexicon):\n",
    "#     pdb.set_trace()\n",
    "    text = text.rstrip(punctuation)    \n",
    "    phones = []\n",
    "    words = re.split(r\"([,;.\\-\\?\\!\\s+])\", text)\n",
    "    for w in words:\n",
    "        if w.lower() in lexicon:\n",
    "            phones += lexicon[w.lower()]\n",
    "        else:\n",
    "            phones += list(filter(lambda p: p != \" \", g2p(w)))\n",
    "    phones = \"{\" + \"}{\".join(phones) + \"}\"\n",
    "    phones = re.sub(r\"\\{[^\\w\\s]?\\}\", \"{spn}\", phones)\n",
    "    phones = phones.replace(\"}{\", \" \")\n",
    "\n",
    "#     print(\"Raw Text Sequence: {}\".format(text))\n",
    "#     print(\"Phoneme Sequence: {}\".format(phones))\n",
    "    sequence = np.array(\n",
    "        text_to_sequence(\n",
    "            phones, preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return np.array(sequence)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fp.to('cuda')\n",
    "# fp.eval()\n",
    "\n",
    "def create_hifigan_finetune_data(records, run= 'train', device='cuda'):    \n",
    "    save_dir = Path(f\"/workspace/nemo/vol/FastSpeech2/hifigan_mels/{run}\")\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    fp.to(device)\n",
    "\n",
    "    # Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
    "    \n",
    "    for i, r in tqdm(enumerate(records)): \n",
    "        with torch.no_grad():\n",
    "            text = r['text']\n",
    "            speaker = r['speaker']\n",
    "            emotion = r['emotion']\n",
    "            ids = raw_texts = [text]\n",
    "            speaker = torch.tensor([speaker]).to(device)\n",
    "            emotion = torch.tensor([emotion]).to(device) \n",
    "            texts = torch.tensor([preprocess_english(text, preprocess_config)]).to(device)\n",
    "            text_lens = torch.tensor([len(texts[0])]).to(device)\n",
    "            max_len = max(text_lens)\n",
    "            batchs = [(speaker, emotion, texts, text_lens, max_len )]\n",
    "            #print(batchs[0])\n",
    "            predictions = fp(*(batchs[0]))\n",
    "            spectrogram = predictions[1].transpose(1, 2)\n",
    "\n",
    "            save_path = save_dir / f\"mel_{i}.npy\"\n",
    "            np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "            r[\"mel_filepath\"] = str(save_path)\n",
    "\n",
    "    hifigan_manifest_path = f\"./hifigan_{run}_ft.json\"\n",
    "    with open(hifigan_manifest_path, \"w\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e98d9b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11903"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a46327e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthesize import preprocess_english\n",
    "\n",
    "# #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fp.to('cuda')\n",
    "# fp.eval()\n",
    "\n",
    "# def create_hifigan_finetune_data(records, run= 'train', device='cuda'):    \n",
    "#     beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "\n",
    "#     save_dir = Path(f\"/workspace/nemo/vol/FastSpeech2/hifigan_mels/{run}\")\n",
    "#     save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "#     fp.to(device)\n",
    "\n",
    "#     # Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
    "#     for i, r in tqdm(enumerate(records)):\n",
    "#         audio = load_wav(r[\"audio_filepath\"])\n",
    "#         audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "#         audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             text = r['text']\n",
    "#             speaker = r['speaker']\n",
    "#             emotion = r['emotion']\n",
    "#             ids = raw_texts = [text[:100]]\n",
    "#             speaker = torch.tensor([speaker]).to(device)\n",
    "#             emotion = torch.tensor([emotion]).to(device) \n",
    "#             texts = torch.tensor([preprocess_english(text, preprocess_config)]).to(device)\n",
    "#             text_lens = torch.tensor([len(texts[0])]).to(device)\n",
    "#             batchs = [(ids, raw_texts, speaker, emotion, texts, text_lens, max(text_lens))]\n",
    "#             #print(batchs[0][2:])\n",
    "#             predictions = fp(*(batchs[0][2:]))\n",
    "#             spectrogram = predictions[1].transpose(1, 2)\n",
    "\n",
    "#     #         if \"normalized_text\" in r:\n",
    "#     #             text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
    "#     #         else:\n",
    "#     #             text = spec_model.parse(r['text'])\n",
    "\n",
    "#     #         text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "#     #         spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "\n",
    "#     #         # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
    "#     #         attn_prior = torch.from_numpy(\n",
    "#     #           beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "#     #         ).unsqueeze(0).to(text.device)\n",
    "\n",
    "#     #         spectrogram = spec_model.forward(\n",
    "#     #           text=text, \n",
    "#     #           input_lens=text_len, \n",
    "#     #           spec=spect, \n",
    "#     #           mel_lens=spect_len, \n",
    "#     #           attn_prior=attn_prior,\n",
    "#     #         )[0]\n",
    "\n",
    "#             save_path = save_dir / f\"mel_{i}.npy\"\n",
    "#             np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "#             r[\"mel_filepath\"] = str(save_path)\n",
    "\n",
    "#     hifigan_manifest_path = f\"./hifigan_{run}_ft.json\"\n",
    "#     with open(hifigan_manifest_path, \"w\") as f:\n",
    "#         for r in records:\n",
    "#             f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5af1026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s][NeMo W 2023-11-25 11:53:29 nemo_logging:349] /tmp/ipykernel_857988/1772628250.py:66: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      texts = torch.tensor([preprocess_english(text, preprocess_config)]).to(device)\n",
      "    \n",
      "10712it [05:17, 33.75it/s]\n"
     ]
    }
   ],
   "source": [
    "create_hifigan_finetune_data(train_rec, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d2335d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1191it [00:35, 33.69it/s]\n"
     ]
    }
   ],
   "source": [
    "create_hifigan_finetune_data(val_rec, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6389b900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: hifigan_conf: No such file or directory\n",
      "/bin/bash: line 0: cd: hifigan_conf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cd hifigan_conf && unzip hifigan.zip\n",
    "! cd hifigan_conf && cd hifigan && wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/conf/hifigan/hifigan.yaml && cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5a67956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n",
      "Copying /root/.cache/torch/NeMo/NeMo_1.19.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo to ./\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "315386678"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_path = !(echo $HOME)\n",
    "home_path = home_path[0]\n",
    "print(home_path)\n",
    "\n",
    "nemo_files = [p for p in Path(f\"{home_path}/.cache/torch/NeMo/\").glob(\"**/tts_hifigan.nemo\")]\n",
    "print(f\"Copying {nemo_files[0]} to ./\")\n",
    "Path(\"./tts_hifigan.nemo\").write_bytes(nemo_files[0].read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56b79eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(home_path)\n\u001b[1;32m      5\u001b[0m nemo_files \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhome_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/.cache/torch/NeMo/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/univnet.nemo\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCopying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnemo_files[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to ./\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Path(\"./tts_hifigan.nemo\").write_bytes(nemo_files[0].read_bytes())\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "home_path = !(echo $HOME)\n",
    "home_path = home_path[0]\n",
    "print(home_path)\n",
    "\n",
    "nemo_files = [p for p in Path(f\"{home_path}/.cache/torch/NeMo/\").glob(\"**/univnet.nemo\")]\n",
    "print(f\"Copying {nemo_files[0]} to ./\")\n",
    "#Path(\"./tts_hifigan.nemo\").write_bytes(nemo_files[0].read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bafe2a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nemo_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f9af1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb -qU\n",
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "423c5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_api_key = \"108bad0089a140932fbbe1c9e2ae182a1a228ffe\"\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4bee8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-25 11:59:55--  https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/hifigan_finetune.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1192 (1.2K) [text/plain]\n",
      "Saving to: ‚Äòhifigan_finetune.py.1‚Äô\n",
      "\n",
      "hifigan_finetune.py 100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-11-25 11:59:55 (80.7 MB/s) - ‚Äòhifigan_finetune.py.1‚Äô saved [1192/1192]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/hifigan_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ac6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n",
      "[NeMo W 2023-11-23 20:33:51 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-23 20:33:51 experimental:27] Module <class 'nemo.collections.tts.models.fastpitch_ssl.FastPitchModel_SSL'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-23 20:33:51 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-23 20:33:51 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "2023-11-23 20:33:52.604229: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[NeMo W 2023-11-23 20:33:55 experimental:27] Module <class 'nemo.collections.tts.models.ssl_tts.SSLDisentangler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-23 20:33:55 experimental:27] Module <class 'nemo.collections.tts.models.vits.VitsModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-11-23 20:33:56 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'hifigan.yaml': Defaults list is missing `_self_`. See https://hydra.cc/docs/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "      warnings.warn(msg, UserWarning)\n",
      "    \n",
      "[NeMo W 2023-11-23 20:33:57 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-11-23 20:33:57 exp_manager:374] Experiments will be logged at hifigan_ft/HifiGan/2023-11-23_20-33-57\n",
      "[NeMo I 2023-11-23 20:33:57 exp_manager:797] TensorboardLogger has been set up\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mconvdev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.16.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1mhifigan_ft/wandb/run-20231123_203400-2023-11-23_20-33-57\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mJosh_emotional_HG\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/convdev/TTS_convai\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/convdev/TTS_convai/runs/2023-11-23_20-33-57\u001b[0m\n",
      "[NeMo I 2023-11-23 20:34:01 exp_manager:812] WandBLogger has been set up\n",
      "[NeMo W 2023-11-23 20:34:01 exp_manager:893] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100000. Please ensure that max_steps will run for at least 5 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2023-11-23 20:34:01 data:1068] Loading dataset from hifigan_train_ft.json.\n",
      "510it [00:00, 56367.63it/s]\n",
      "[NeMo I 2023-11-23 20:34:01 data:1092] Loaded dataset with 510 files.\n",
      "[NeMo I 2023-11-23 20:34:01 data:1094] Dataset contains 1.28 hours.\n",
      "[NeMo I 2023-11-23 20:34:01 data:376] Pruned 0 files. Final dataset contains 510 files\n",
      "[NeMo I 2023-11-23 20:34:01 data:378] Pruned 0.00 hours. Final dataset contains 1.28 hours.\n",
      "[NeMo I 2023-11-23 20:34:01 data:1068] Loading dataset from hifigan_val_ft.json.\n",
      "510it [00:00, 48956.26it/s]\n",
      "[NeMo I 2023-11-23 20:34:01 data:1092] Loaded dataset with 510 files.\n",
      "[NeMo I 2023-11-23 20:34:01 data:1094] Dataset contains 1.28 hours.\n",
      "[NeMo I 2023-11-23 20:34:01 data:376] Pruned 33 files. Final dataset contains 477 files\n",
      "[NeMo I 2023-11-23 20:34:01 data:378] Pruned 0.02 hours. Final dataset contains 1.26 hours.\n",
      "[NeMo I 2023-11-23 20:34:01 features:291] PADDING: 0\n",
      "[NeMo I 2023-11-23 20:34:01 features:299] STFT using exact pad\n",
      "[NeMo I 2023-11-23 20:34:01 features:291] PADDING: 0\n",
      "[NeMo I 2023-11-23 20:34:01 features:299] STFT using exact pad\n",
      "[NeMo W 2023-11-23 20:34:08 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/train_finetune.txt\n",
      "      min_duration: 0.75\n",
      "      n_segments: 8192\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: true\n",
      "      batch_size: 64\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2023-11-23 20:34:08 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    dataset:\n",
      "      _target_: nemo.collections.tts.data.datalayers.MelAudioDataset\n",
      "      manifest_filepath: /home/fkreuk/data/val_finetune.txt\n",
      "      min_duration: 3\n",
      "      n_segments: 66150\n",
      "    dataloader_params:\n",
      "      drop_last: false\n",
      "      shuffle: false\n",
      "      batch_size: 5\n",
      "      num_workers: 4\n",
      "    \n",
      "[NeMo W 2023-11-23 20:34:08 features:268] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2023-11-23 20:34:08 features:291] PADDING: 0\n",
      "[NeMo W 2023-11-23 20:34:08 features:268] Using torch_stft is deprecated and has been removed. The values have been forcibly set to False for FilterbankFeatures and AudioToMelSpectrogramPreprocessor. Please set exact_pad to True as needed.\n",
      "[NeMo I 2023-11-23 20:34:08 features:291] PADDING: 0\n",
      "[NeMo I 2023-11-23 20:34:12 save_restore_connector:249] Model HifiGanModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--tts_hifigan/snapshots/3ba1fed954276287015654bf4c78060ffc9a4772/tts_hifigan.nemo.\n",
      "[NeMo I 2023-11-23 20:34:12 modelPT:1268] Model checkpoint restored from pretrained checkpoint with name : `nvidia/tts_hifigan`\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n"
     ]
    }
   ],
   "source": [
    "!(HYDRA_FULL_ERROR=1 python hifigan_finetune.py \\\n",
    "--config-name=hifigan.yaml \\\n",
    "model.train_ds.dataloader_params.batch_size=16 \\\n",
    "model.max_steps=100000 \\\n",
    "model.optim.lr=1e-4 \\\n",
    "~model.optim.sched \\\n",
    "train_dataset=hifigan_train_ft.json \\\n",
    "validation_datasets=hifigan_val_ft.json \\\n",
    "exp_manager.exp_dir=hifigan_ft \\\n",
    "+init_from_pretrained_model=nvidia/tts_hifigan \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "trainer.log_every_n_steps=3 \\\n",
    "exp_manager.create_wandb_logger=true \\\n",
    "exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \\\n",
    ")\n",
    "# exp_manager.create_wandb_logger=true \\\n",
    "# exp_manager.wandb_logger_kwargs.name='Josh_emotional_HG' \\\n",
    "# exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \\\n",
    "# model/train_ds=train_ds_finetune \\\n",
    "# model/validation_ds=val_ds_finetune \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed8f6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.torch.data import VocoderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add4e7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYDRA_FULL_ERROR=1 python hifigan_finetune.py --config-name=hifigan.yaml model.train_ds.dataloader_params.batch_size=32 model.max_steps=100000 model.optim.lr=1e-4 ~model.optim.sched train_dataset=hifigan_train_ft.json validation_datasets=hifigan_val_ft.json exp_manager.exp_dir=hifigan_ft +init_from_pretrained_model=nvidia/tts_hifigan trainer.check_val_every_n_epoch=5 trainer.log_every_n_steps=3 exp_manager.create_wandb_logger=true exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"HYDRA_FULL_ERROR=1 python hifigan_finetune.py \\\n",
    "--config-name=hifigan.yaml \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.max_steps=100000 \\\n",
    "model.optim.lr=1e-4 \\\n",
    "~model.optim.sched \\\n",
    "train_dataset=hifigan_train_ft.json \\\n",
    "validation_datasets=hifigan_val_ft.json \\\n",
    "exp_manager.exp_dir=hifigan_ft \\\n",
    "+init_from_pretrained_model=nvidia/tts_hifigan \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "trainer.log_every_n_steps=3 \\\n",
    "exp_manager.create_wandb_logger=true \\\n",
    "exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6095d199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYDRA_FULL_ERROR=1 python hifigan_finetune.py --config-name=hifigan.yaml model.train_ds.dataloader_params.batch_size=32 model.max_steps=100000 model.optim.lr=1e-4 ~model.optim.sched train_dataset=hifigan_train_ft.json validation_datasets=hifigan_val_ft.json exp_manager.exp_dir=hifigan_ft +init_from_ptl_ckpt=/workspace/nemo/vol/FastSpeech2/hifiga02781n.ckpt trainer.check_val_every_n_epoch=5 trainer.log_every_n_steps=3 exp_manager.create_wandb_logger=true exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"HYDRA_FULL_ERROR=1 python hifigan_finetune.py \\\n",
    "--config-name=hifigan.yaml \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.max_steps=100000 \\\n",
    "model.optim.lr=1e-4 \\\n",
    "~model.optim.sched \\\n",
    "train_dataset=hifigan_train_ft.json \\\n",
    "validation_datasets=hifigan_val_ft.json \\\n",
    "exp_manager.exp_dir=hifigan_ft \\\n",
    "+init_from_ptl_ckpt=/workspace/nemo/vol/FastSpeech2/hifiga02781n.ckpt \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "trainer.log_every_n_steps=3 \\\n",
    "exp_manager.create_wandb_logger=true \\\n",
    "exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f92b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
