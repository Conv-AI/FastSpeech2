{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a4f2e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-12-20 01:32:42 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-12-20 01:32:42 experimental:27] Module <class 'nemo.collections.tts.models.fastpitch_ssl.FastPitchModel_SSL'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-12-20 01:32:42 experimental:27] Module <class 'nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-12-20 01:32:42 experimental:27] Module <class 'nemo.collections.tts.models.radtts.RadTTSModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-12-20 01:32:42 experimental:27] Module <class 'nemo.collections.tts.models.ssl_tts.SSLDisentangler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-12-20 01:32:42 experimental:27] Module <class 'nemo.collections.tts.models.vits.VitsModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from model import FastSpeech2\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import soundfile as sf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0632128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "base_path = '/workspace/nemo/vol/vol2/'\n",
    "dataset = 'NovaConvai'\n",
    "\n",
    "model_config = yaml.load(open(f'{base_path}FastSpeech2/config/{dataset}/model.yaml',\n",
    "                              \"r\"), Loader=yaml.FullLoader)\n",
    "preprocess_config = yaml.load(open(f'{base_path}FastSpeech2/config/{dataset}/preprocess.yaml',\n",
    "                              \"r\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462304e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FastSpeech2(model_config=model_config, preprocess_config=preprocess_config)\n",
    "ckpt_file_path = '/workspace/nemo/vol/vol5/nova_original_output/ckpt/NovaConvai/60000.pth.tar'\n",
    "checkpoint = torch.load(ckpt_file_path, map_location=torch.device('cuda'))\n",
    "fp.load_state_dict(checkpoint['model'])\n",
    "fp = fp.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb3a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# fp = nn.DataParallel(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478a1282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint['model']['emotion_emb.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3985ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def load_wav(audio_file, target_sr=None):\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "        sample_rate = f.samplerate\n",
    "        if target_sr is not None and target_sr != sample_rate:\n",
    "            samples = librosa.core.resample(samples, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    return samples.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e75abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('/workspace/nemo/vol/LBM_EB/preprocessed_data/LBM_EB/paths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d16c8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rec = []\n",
    "\n",
    "# speakers_path = \"/workspace/nemo/vol/LBM_EB/preprocessed_data/LBM_EB/speakers.json\"\n",
    "# emotions_path = \"/workspace/nemo/vol/LBM_EB/preprocessed_data/LBM_EB/emotions.json\"\n",
    "\n",
    "# with open(speakers_path, \"r\") as f:\n",
    "#     speakers = json.load(f)\n",
    "    \n",
    "# with open(emotions_path, \"r\") as f:\n",
    "#     emotions = json.load(f)\n",
    "\n",
    "# for i, row in tqdm(df[886248:].iterrows()):\n",
    "#     text_path = row.text_paths\n",
    "#     wav_path = row.wav_paths\n",
    "#     try:\n",
    "#         with open(text_path, 'r') as f:\n",
    "#             text = f.read()\n",
    "#         duration = librosa.get_duration(filename=wav_path)\n",
    "#         speaker = text_path.split('/')[-3]\n",
    "#         emotion = text_path.split('/')[-2]\n",
    "#         speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "#         emotion_ = [v for k,v in emotions.items() if k==emotion][0] \n",
    "\n",
    "#         r = {\n",
    "#            \"audio_filepath\" : wav_path,\n",
    "#            \"text\" : text,\n",
    "#            \"speaker\": speaker_,\n",
    "#            \"emotion\": emotion_,\n",
    "#            \"duration\" : round(duration, 1),\n",
    "#            \"text_no_preprocessing\" : text\n",
    "#         }\n",
    "#         rec.append(r)\n",
    "#     except:\n",
    "#         continue\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c12943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16dd19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Sample array\n",
    "# #data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# # Serialize the array to a JSON-formatted string\n",
    "# json_data = json.dumps(rec)\n",
    "\n",
    "# # Write the JSON string to a file\n",
    "# with open('data_rec.json', 'w') as file:\n",
    "#     file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99738c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('data_rec.json', 'r') as file:\n",
    "#     rec = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2469d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac01780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a2a5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18080it [00:01, 9803.49it/s] \n"
     ]
    }
   ],
   "source": [
    "# file_paths = list()\n",
    "rec = []\n",
    "\n",
    "voices_path = '/workspace/nemo/vol/vol2/nova_mod_audio'\n",
    "base_path = '/workspace/nemo/vol/vol5/14122023Dataset/RawDataAligned_modified_wav/'\n",
    "base_path_lab = '/workspace/nemo/vol/vol5/Dataset/RawDataAligned/'\n",
    "\n",
    "preprocess_dir = \"/workspace/nemo/vol/vol5/14122023Dataset/preprocessed_data/\"\n",
    "speakers_path = \"/workspace/nemo/vol/vol5/14122023Dataset/preprocessed_data/speakers.json\"\n",
    "emotions_path = \"/workspace/nemo/vol/vol5/14122023Dataset/preprocessed_data/emotions.json\"\n",
    "\n",
    "with open(preprocess_dir+'train.txt', 'r') as f:\n",
    "    for line in tqdm(f):\n",
    "        file, speaker, emotion, ph, text = line.split('|')\n",
    "        if not speaker == 'NovaConvai':\n",
    "            continue\n",
    "        audio_filepath = os.path.join(voices_path, '.'.join([file, 'wav']))\n",
    "        audio_filepath_verify = os.path.join(base_path, speaker, emotion, '.'.join([file, 'wav']))\n",
    "        text_filepath = os.path.join(base_path_lab, speaker, emotion, '.'.join([file, 'lab']))\n",
    "        if not os.path.exists(audio_filepath) or not os.path.exists(text_filepath):\n",
    "            continue\n",
    "#         file_paths.append((audio_filepath, text_filepath))\n",
    "        duration = librosa.get_duration(filename=audio_filepath)\n",
    "        with open(text_filepath, 'r') as f:\n",
    "            text = f.read()\n",
    "        with open(speakers_path, \"r\") as f:\n",
    "            speakers = json.load(f)\n",
    "            speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "        with open(emotions_path, \"r\") as f:\n",
    "            emotions = json.load(f)\n",
    "            emotion_ = [v for k,v in emotions.items() if k==emotion][0]   \n",
    "        r = {\n",
    "           \"audio_filepath\" : audio_filepath,\n",
    "           \"text\" : text,\n",
    "           \"speaker\": speaker_,\n",
    "           \"emotion\": emotion_,\n",
    "           \"duration\" : round(duration,1),\n",
    "           \"text_no_preprocessing\" : text\n",
    "        }\n",
    "        rec.append(r)\n",
    "        \n",
    "with open(preprocess_dir+'val.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        file, speaker, emotion, ph, text = line.split('|')\n",
    "        if not speaker == 'NovaConvai':\n",
    "            continue\n",
    "        audio_filepath = os.path.join(voices_path, '.'.join([file, 'wav']))\n",
    "        audio_filepath_verify = os.path.join(base_path, speaker, emotion, '.'.join([file, 'wav']))\n",
    "        text_filepath = os.path.join(base_path_lab, speaker, emotion, '.'.join([file, 'lab']))\n",
    "        if not os.path.exists(audio_filepath) or not os.path.exists(text_filepath):\n",
    "            continue\n",
    "#         file_paths.append((audio_filepath, text_filepath))\n",
    "        duration = librosa.get_duration(filename=audio_filepath)\n",
    "        with open(text_filepath, 'r') as f:\n",
    "            text = f.read()\n",
    "        with open(speakers_path, \"r\") as f:\n",
    "            speakers = json.load(f)\n",
    "            speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "        with open(emotions_path, \"r\") as f:\n",
    "            emotions = json.load(f)\n",
    "            emotion_ = [v for k,v in emotions.items() if k==emotion][0]   \n",
    "        r = {\n",
    "           \"audio_filepath\" : audio_filepath,\n",
    "           \"text\" : text,\n",
    "           \"speaker\": speaker_,\n",
    "           \"emotion\": emotion_,\n",
    "           \"duration\" : round(duration,1),\n",
    "           \"text_no_preprocessing\" : text\n",
    "        }\n",
    "        rec.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8e6a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'audio_filepath': '/workspace/nemo/vol/vol2/nova_mod_audio/ed5db4878530b16a609bfe4f4f015bafc9eacb2f446187b470eb3caff2a81763_chunk_748.wav',\n",
       "  'text': 'but liam, he he wants your ram wants to copy you.',\n",
       "  'speaker': 81,\n",
       "  'emotion': 11,\n",
       "  'duration': 4.2,\n",
       "  'text_no_preprocessing': 'but liam, he he wants your ram wants to copy you.'},\n",
       " 3398)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec[0], len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "009d8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voices_path = '/workspace/nemo/vol/extvol3/NovaEmo/RawData/'\n",
    "\n",
    "# base_path = voices_path\n",
    "\n",
    "# speakers_path = \"/workspace/nemo/vol/extvol3/NovaEmo/preprocessed_data/speakers.json\"\n",
    "# emotions_path = \"/workspace/nemo/vol/extvol3/NovaEmo/preprocessed_data/emotions.json\"\n",
    "\n",
    "# rec = []\n",
    "\n",
    "# # speaker = 'Female-V2'\n",
    "# covered= []\n",
    "\n",
    "# for speaker in os.listdir(base_path)\n",
    "\n",
    "#     for emotion in os.listdir(base_path):\n",
    "#         emotion_path = os.path.join(base_path, emotion)\n",
    "#         print(emotion_path)\n",
    "\n",
    "#         for file in os.listdir(emotion_path):\n",
    "#             try:\n",
    "#                 filename = file.split('.')[0]\n",
    "#                 if filename in covered:\n",
    "#                     continue\n",
    "#                 covered.append(filename)\n",
    "\n",
    "#                 audio_filepath = os.path.join(emotion_path, '.'.join([filename, 'wav'])) \n",
    "#                 text_filepath = os.path.join(emotion_path, '.'.join([filename, 'lab']))\n",
    "#                 #print(text_filepath)\n",
    "#                 duration = librosa.get_duration(filename=audio_filepath)\n",
    "#                 with open(text_filepath, 'r') as f:\n",
    "#                     text = f.read()\n",
    "#                 with open(speakers_path, \"r\") as f:\n",
    "#                     speakers = json.load(f)\n",
    "#                     speaker_ = [v for k,v in speakers.items() if k==speaker][0]\n",
    "#                 with open(emotions_path, \"r\") as f:\n",
    "#                     emotions = json.load(f)\n",
    "#                     emotion_ = [v for k,v in emotions.items() if k==emotion][0]   \n",
    "#                 r = {\n",
    "#                    \"audio_filepath\" : audio_filepath,\n",
    "#                    \"text\" : text,\n",
    "#                    \"speaker\": speaker_,\n",
    "#                    \"emotion\": emotion_,\n",
    "#                    \"duration\" : round(duration,1),\n",
    "#                    \"text_no_preprocessing\" : text\n",
    "#                 }\n",
    "#                 rec.append(r)\n",
    "#             except:\n",
    "#                 continue\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d888cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(rec)\n",
    "train_len = int(0.9*len(rec))\n",
    "train_rec = rec[:train_len]\n",
    "val_rec = rec[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "830beace",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = f'./fastpitch_train.json'\n",
    "with open(train_manifest, \"w\") as f:\n",
    "    for s in train_rec:\n",
    "        f.write(json.dumps(s) + '\\n')\n",
    "        \n",
    "val_manifest = f'./fastpitch_val.json'\n",
    "with open(val_manifest, \"w\") as f:\n",
    "    for s in val_rec:\n",
    "        f.write(json.dumps(s) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d80e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_rec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da074d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_filepath': '/workspace/nemo/vol/vol2/nova_mod_audio/14ab44ae6eb362b3408f5ea1da7e8625d8b0424fc0ac9f4e52a850d15f3ec09f_chunk_1817.wav',\n",
       " 'text': \"you're shaking like a loose exhaust pipe.\",\n",
       " 'speaker': 81,\n",
       " 'emotion': 11,\n",
       " 'duration': 1.9,\n",
       " 'text_no_preprocessing': \"you're shaking like a loose exhaust pipe.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rec[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19f1e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "from string import punctuation\n",
    "import re\n",
    "from text import text_to_sequence\n",
    "\n",
    "\n",
    "def read_lexicon(lex_path):\n",
    "    lexicon = {}\n",
    "    with open(lex_path) as f:\n",
    "        for line in f:\n",
    "            temp = re.split(r\"\\s+\", line.strip(\"\\n\"))\n",
    "            word = temp[0]\n",
    "            phones = temp[1:]\n",
    "            if word.lower() not in lexicon:\n",
    "                lexicon[word.lower()] = phones\n",
    "    return lexicon\n",
    "\n",
    "g2p = G2p()\n",
    "lexicon = read_lexicon(preprocess_config[\"path\"][\"lexicon_path\"])\n",
    "\n",
    "def preprocess_english(text, preprocess_config, g2p = g2p, lexicon = lexicon):\n",
    "#     pdb.set_trace()\n",
    "    text = text.rstrip(punctuation)    \n",
    "    phones = []\n",
    "    words = re.split(r\"([,;.\\-\\?\\!\\s+])\", text)\n",
    "    for w in words:\n",
    "        if w.lower() in lexicon:\n",
    "            phones += lexicon[w.lower()]\n",
    "        else:\n",
    "            phones += list(filter(lambda p: p != \" \", g2p(w)))\n",
    "    phones = \"{\" + \"}{\".join(phones) + \"}\"\n",
    "    phones = re.sub(r\"\\{[^\\w\\s]?\\}\", \"{spn}\", phones)\n",
    "    phones = phones.replace(\"}{\", \" \")\n",
    "\n",
    "#     print(\"Raw Text Sequence: {}\".format(text))\n",
    "#     print(\"Phoneme Sequence: {}\".format(phones))\n",
    "    sequence = np.array(\n",
    "        text_to_sequence(\n",
    "            phones, preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return np.array(sequence)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fp.to('cuda')\n",
    "# fp.eval()\n",
    "\n",
    "def create_hifigan_finetune_data(records, run= 'train', device='cuda'):    \n",
    "    save_dir = Path(f\"/workspace/nemo/vol/FastSpeech2/hifigan_mels/{run}\")\n",
    "    save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    fp.to(device)\n",
    "\n",
    "    # Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
    "    \n",
    "    for i, r in tqdm(enumerate(records)): \n",
    "        with torch.no_grad():\n",
    "            text = r['text']\n",
    "            speaker = r['speaker']\n",
    "            emotion = r['emotion']\n",
    "            ids = raw_texts = [text]\n",
    "            speaker = torch.tensor([speaker]).to(device)\n",
    "            emotion = torch.tensor([emotion]).to(device) \n",
    "            texts = torch.tensor([preprocess_english(text, preprocess_config)]).to(device)\n",
    "            text_lens = torch.tensor([len(texts[0])]).to(device)\n",
    "            max_len = max(text_lens)\n",
    "            batchs = [(speaker, emotion, texts, text_lens, max_len )]\n",
    "            #print(batchs[0])\n",
    "            predictions = fp(*(batchs[0]))\n",
    "            spectrogram = predictions[1].transpose(1, 2)\n",
    "\n",
    "            save_path = save_dir / f\"mel_{i}.npy\"\n",
    "            np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "            r[\"mel_filepath\"] = str(save_path)\n",
    "\n",
    "    hifigan_manifest_path = f\"./hifigan_{run}_ft.json\"\n",
    "    with open(hifigan_manifest_path, \"w\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "461e0c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3398"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8d3f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from synthesize import preprocess_english\n",
    "\n",
    "# #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# fp.to('cuda')\n",
    "# fp.eval()\n",
    "\n",
    "# def create_hifigan_finetune_data(records, run= 'train', device='cuda'):    \n",
    "#     beta_binomial_interpolator = BetaBinomialInterpolator()\n",
    "\n",
    "#     save_dir = Path(f\"/workspace/nemo/vol/FastSpeech2/hifigan_mels/{run}\")\n",
    "#     save_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "#     fp.to(device)\n",
    "\n",
    "#     # Generate a spectrograms (we need to use ground truth alignment for correct matching between audio and mels)\n",
    "#     for i, r in tqdm(enumerate(records)):\n",
    "#         audio = load_wav(r[\"audio_filepath\"])\n",
    "#         audio = torch.from_numpy(audio).unsqueeze(0).to(device)\n",
    "#         audio_len = torch.tensor(audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             text = r['text']\n",
    "#             speaker = r['speaker']\n",
    "#             emotion = r['emotion']\n",
    "#             ids = raw_texts = [text[:100]]\n",
    "#             speaker = torch.tensor([speaker]).to(device)\n",
    "#             emotion = torch.tensor([emotion]).to(device) \n",
    "#             texts = torch.tensor([preprocess_english(text, preprocess_config)]).to(device)\n",
    "#             text_lens = torch.tensor([len(texts[0])]).to(device)\n",
    "#             batchs = [(ids, raw_texts, speaker, emotion, texts, text_lens, max(text_lens))]\n",
    "#             #print(batchs[0][2:])\n",
    "#             predictions = fp(*(batchs[0][2:]))\n",
    "#             spectrogram = predictions[1].transpose(1, 2)\n",
    "\n",
    "#     #         if \"normalized_text\" in r:\n",
    "#     #             text = spec_model.parse(r[\"normalized_text\"], normalize=False)\n",
    "#     #         else:\n",
    "#     #             text = spec_model.parse(r['text'])\n",
    "\n",
    "#     #         text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "#     #         spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len)\n",
    "\n",
    "#     #         # Generate attention prior and spectrogram inputs for HiFi-GAN\n",
    "#     #         attn_prior = torch.from_numpy(\n",
    "#     #           beta_binomial_interpolator(spect_len.item(), text_len.item())\n",
    "#     #         ).unsqueeze(0).to(text.device)\n",
    "\n",
    "#     #         spectrogram = spec_model.forward(\n",
    "#     #           text=text, \n",
    "#     #           input_lens=text_len, \n",
    "#     #           spec=spect, \n",
    "#     #           mel_lens=spect_len, \n",
    "#     #           attn_prior=attn_prior,\n",
    "#     #         )[0]\n",
    "\n",
    "#             save_path = save_dir / f\"mel_{i}.npy\"\n",
    "#             np.save(save_path, spectrogram[0].to('cpu').numpy())\n",
    "#             r[\"mel_filepath\"] = str(save_path)\n",
    "\n",
    "#     hifigan_manifest_path = f\"./hifigan_{run}_ft.json\"\n",
    "#     with open(hifigan_manifest_path, \"w\") as f:\n",
    "#         for r in records:\n",
    "#             f.write(json.dumps(r) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05fc4c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s][NeMo W 2023-12-20 01:48:08 nemo_logging:349] /tmp/ipykernel_1014421/1772628250.py:66: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "      texts = torch.tensor([preprocess_english(text, preprocess_config)]).to(device)\n",
      "    \n",
      "3058it [02:07, 24.06it/s]\n"
     ]
    }
   ],
   "source": [
    "create_hifigan_finetune_data(train_rec, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9690165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "340it [00:12, 26.27it/s]\n"
     ]
    }
   ],
   "source": [
    "create_hifigan_finetune_data(val_rec, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24d94fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: hifigan_conf: No such file or directory\n",
      "/bin/bash: line 0: cd: hifigan_conf: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! cd hifigan_conf && unzip hifigan.zip\n",
    "! cd hifigan_conf && cd hifigan && wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/conf/hifigan/hifigan.yaml && cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "579a8a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n",
      "Copying /root/.cache/torch/NeMo/NeMo_1.19.0/tts_hifigan/e6da322f0f7e7dcf3f1900a9229a7e69/tts_hifigan.nemo to ./\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "315386678"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_path = !(echo $HOME)\n",
    "home_path = home_path[0]\n",
    "print(home_path)\n",
    "\n",
    "nemo_files = [p for p in Path(f\"{home_path}/.cache/torch/NeMo/\").glob(\"**/tts_hifigan.nemo\")]\n",
    "print(f\"Copying {nemo_files[0]} to ./\")\n",
    "Path(\"./tts_hifigan.nemo\").write_bytes(nemo_files[0].read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd0e725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "home_path = !(echo $HOME)\n",
    "home_path = home_path[0]\n",
    "print(home_path)\n",
    "\n",
    "# nemo_files = [p for p in Path(f\"{home_path}/.cache/torch/NeMo/\").glob(\"**/univnet.nemo\")]\n",
    "# print(f\"Copying {nemo_files[0]} to ./\")\n",
    "#Path(\"./tts_hifigan.nemo\").write_bytes(nemo_files[0].read_bytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dad7abc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nemo_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "844a34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb -qU\n",
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59f7001",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_api_key = \"108bad0089a140932fbbe1c9e2ae182a1a228ffe\"\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8d320eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-20 01:51:55--  https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/hifigan_finetune.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1192 (1.2K) [text/plain]\n",
      "Saving to: ‘hifigan_finetune.py.6’\n",
      "\n",
      "hifigan_finetune.py 100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
      "\n",
      "2023-12-20 01:51:55 (66.9 MB/s) - ‘hifigan_finetune.py.6’ saved [1192/1192]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/nvidia/NeMo/main/examples/tts/hifigan_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f105e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !(HYDRA_FULL_ERROR=1 python hifigan_finetune.py \\\n",
    "# --config-name=hifigan.yaml \\\n",
    "# model.train_ds.dataloader_params.batch_size=16 \\\n",
    "# model.max_steps=100000 \\\n",
    "# model.optim.lr=1e-4 \\\n",
    "# ~model.optim.sched \\\n",
    "# train_dataset=hifigan_train_ft.json \\\n",
    "# validation_datasets=hifigan_val_ft.json \\\n",
    "# exp_manager.exp_dir=hifigan_ft \\\n",
    "# +init_from_pretrained_model=nvidia/tts_hifigan \\\n",
    "# trainer.check_val_every_n_epoch=5 \\\n",
    "# trainer.log_every_n_steps=3 \\\n",
    "# exp_manager.create_wandb_logger=true \\\n",
    "# exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' \\\n",
    "# exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \\\n",
    "# )\n",
    "# # exp_manager.create_wandb_logger=true \\\n",
    "# # exp_manager.wandb_logger_kwargs.name='Josh_emotional_HG' \\\n",
    "# # exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \\\n",
    "# # model/train_ds=train_ds_finetune \\\n",
    "# # model/validation_ds=val_ds_finetune \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9eced9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.torch.data import VocoderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "840379af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYDRA_FULL_ERROR=1 python hifigan_finetune.py --config-name=hifigan.yaml model.train_ds.dataloader_params.batch_size=32 model.max_steps=100000 model.optim.lr=1e-4 ~model.optim.sched train_dataset=hifigan_train_ft.json validation_datasets=hifigan_val_ft.json exp_manager.exp_dir=hifigan_ft +init_from_pretrained_model=tts_en_hifigan trainer.check_val_every_n_epoch=5 trainer.log_every_n_steps=3 exp_manager.create_wandb_logger=true exp_manager.wandb_logger_kwargs.name='BS_32' exp_manager.wandb_logger_kwargs.project=\"Nova_Emo_HG\"  \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"HYDRA_FULL_ERROR=1 python hifigan_finetune.py \\\n",
    "--config-name=hifigan.yaml \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.max_steps=100000 \\\n",
    "model.optim.lr=1e-4 \\\n",
    "~model.optim.sched \\\n",
    "train_dataset=hifigan_train_ft.json \\\n",
    "validation_datasets=hifigan_val_ft.json \\\n",
    "exp_manager.exp_dir=hifigan_ft \\\n",
    "+init_from_pretrained_model=tts_en_hifigan \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "trainer.log_every_n_steps=3 \\\n",
    "exp_manager.create_wandb_logger=true \\\n",
    "exp_manager.wandb_logger_kwargs.name='BS_32' \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"Nova_Emo_HG\"  \\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "681f516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYDRA_FULL_ERROR=1 python hifigan_finetune.py --config-name=hifigan.yaml model.train_ds.dataloader_params.batch_size=32 model.max_steps=100000 model.optim.lr=1e-4 ~model.optim.sched train_dataset=hifigan_train_ft.json validation_datasets=hifigan_val_ft.json exp_manager.exp_dir=hifigan_ft +init_from_ptl_ckpt=/workspace/nemo/vol/FastSpeech2/hifiga02781n.ckpt trainer.check_val_every_n_epoch=5 trainer.log_every_n_steps=3 exp_manager.create_wandb_logger=true exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"HYDRA_FULL_ERROR=1 python hifigan_finetune.py \\\n",
    "--config-name=hifigan.yaml \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.max_steps=100000 \\\n",
    "model.optim.lr=1e-4 \\\n",
    "~model.optim.sched \\\n",
    "train_dataset=hifigan_train_ft.json \\\n",
    "validation_datasets=hifigan_val_ft.json \\\n",
    "exp_manager.exp_dir=hifigan_ft \\\n",
    "+init_from_ptl_ckpt=/workspace/nemo/vol/FastSpeech2/hifiga02781n.ckpt \\\n",
    "trainer.check_val_every_n_epoch=5 \\\n",
    "trainer.log_every_n_steps=3 \\\n",
    "exp_manager.create_wandb_logger=true \\\n",
    "exp_manager.wandb_logger_kwargs.name='Nova_Emo_HG' \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"TTS_convai\"  \\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860854a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
