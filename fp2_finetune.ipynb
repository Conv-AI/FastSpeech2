{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eda91c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import FastSpeech2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0ba55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explore_ckpt_file(ckpt_path):\n",
    "    try:\n",
    "        # Load the checkpoint data\n",
    "        checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
    "\n",
    "        # List all keys in the checkpoint dictionary (usually contains 'model_state_dict' and more)\n",
    "        print(\"Keys in the checkpoint dictionary:\\n\")\n",
    "        for key in checkpoint.keys():\n",
    "            print(key)\n",
    "\n",
    "        # Access the model's state_dict (modify 'model_state_dict' if different key)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "            # List all keys in the state_dict (these are the model's parameter names)\n",
    "            print(\"\\nKeys in the model's state_dict:\\n\")\n",
    "            for key in state_dict.keys():\n",
    "                print(key)\n",
    "\n",
    "            # Access specific parameters (you can modify these to explore the data)\n",
    "            print(\"\\nExample: Accessing specific parameters:\\n\")\n",
    "            parameter_name = \"your_parameter_name_here\"  # Change this to a specific parameter name\n",
    "            if parameter_name in state_dict:\n",
    "                parameter = state_dict[parameter_name]\n",
    "                print(f\"Parameter: {parameter_name}\")\n",
    "                print(f\"Shape: {parameter.shape}\")\n",
    "                print(f\"Data: {parameter}\")\n",
    "            else:\n",
    "                print(f\"Parameter {parameter_name} not found in the state_dict.\")\n",
    "        else:\n",
    "            print(\"The 'model_state_dict' key not found in the checkpoint.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    # Replace 'path/to/your/checkpoint.ckpt' with the actual path to your .ckpt file\n",
    "ckpt_file_path = './output/ckpt/LBM_EB/119000.pth.tar'\n",
    "#explore_ckpt_file(ckpt_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f20a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(ckpt_file_path, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7517731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "model_config = yaml.load(open('./config/emovdb_frozen/model.yaml',\n",
    "                              \"r\"), Loader=yaml.FullLoader)\n",
    "preprocess_config = yaml.load(open('./config/emovdb_frozen/preprocess.yaml',\n",
    "                              \"r\"), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d52d1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c246a3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = FastSpeech2(model_config=model_config, preprocess_config=preprocess_config)\n",
    "# #ckpt_file_path = '/workspace/nemo/vol/FastSpeech2/output/ckpt/RAVDESS/800000.pth.tar'\n",
    "# # checkpoint = torch.load(ckpt_file_path, map_location=torch.device('cuda'))\n",
    "# checkpoint = torch.load(ckpt_file_path, map_location=torch.device('cpu'))\n",
    "# fp.load_state_dict(checkpoint['model'])\n",
    "# fp = fp.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41bc2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_speakers = checkpoint['model']['speaker_emb.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f971f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_tensor = torch.randn(5, 512)\n",
    "speaker_tensor = torch.randn(4, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ebf9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_emotions = checkpoint['model']['emotion_emb.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf932a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_speakers = torch.cat((old_speakers, speaker_tensor), dim=0)\n",
    "new_emotions = torch.cat((old_emotions, emotion_tensor), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9e5fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1906, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_speakers.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2a4217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_emotions.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2057302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['model']['speaker_emb.weight'] =  speaker_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9afb597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['model']['emotion_emb.weight'] = emotion_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "251c44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = FastSpeech2(model_config=model_config, preprocess_config=preprocess_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25595c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c20e5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(checkpoint, 'EMOVDB_frozen_119000.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "579f5f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_emb.weight True\n",
      "emotion_emb.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in fp.named_parameters():\n",
    "    if 'speaker' in name or 'emotion' in name:\n",
    "        print(name, param.requires_grad)\n",
    "        continue\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9c46864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.position_enc False\n",
      "encoder.src_word_emb.weight False\n",
      "encoder.layer_stack.0.slf_attn.w_qs.weight False\n",
      "encoder.layer_stack.0.slf_attn.w_qs.bias False\n",
      "encoder.layer_stack.0.slf_attn.w_ks.weight False\n",
      "encoder.layer_stack.0.slf_attn.w_ks.bias False\n",
      "encoder.layer_stack.0.slf_attn.w_vs.weight False\n",
      "encoder.layer_stack.0.slf_attn.w_vs.bias False\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.weight False\n",
      "encoder.layer_stack.0.slf_attn.layer_norm.bias False\n",
      "encoder.layer_stack.0.slf_attn.fc.weight False\n",
      "encoder.layer_stack.0.slf_attn.fc.bias False\n",
      "encoder.layer_stack.0.pos_ffn.w_1.weight False\n",
      "encoder.layer_stack.0.pos_ffn.w_1.bias False\n",
      "encoder.layer_stack.0.pos_ffn.w_2.weight False\n",
      "encoder.layer_stack.0.pos_ffn.w_2.bias False\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.weight False\n",
      "encoder.layer_stack.0.pos_ffn.layer_norm.bias False\n",
      "encoder.layer_stack.1.slf_attn.w_qs.weight False\n",
      "encoder.layer_stack.1.slf_attn.w_qs.bias False\n",
      "encoder.layer_stack.1.slf_attn.w_ks.weight False\n",
      "encoder.layer_stack.1.slf_attn.w_ks.bias False\n",
      "encoder.layer_stack.1.slf_attn.w_vs.weight False\n",
      "encoder.layer_stack.1.slf_attn.w_vs.bias False\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.weight False\n",
      "encoder.layer_stack.1.slf_attn.layer_norm.bias False\n",
      "encoder.layer_stack.1.slf_attn.fc.weight False\n",
      "encoder.layer_stack.1.slf_attn.fc.bias False\n",
      "encoder.layer_stack.1.pos_ffn.w_1.weight False\n",
      "encoder.layer_stack.1.pos_ffn.w_1.bias False\n",
      "encoder.layer_stack.1.pos_ffn.w_2.weight False\n",
      "encoder.layer_stack.1.pos_ffn.w_2.bias False\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.weight False\n",
      "encoder.layer_stack.1.pos_ffn.layer_norm.bias False\n",
      "encoder.layer_stack.2.slf_attn.w_qs.weight False\n",
      "encoder.layer_stack.2.slf_attn.w_qs.bias False\n",
      "encoder.layer_stack.2.slf_attn.w_ks.weight False\n",
      "encoder.layer_stack.2.slf_attn.w_ks.bias False\n",
      "encoder.layer_stack.2.slf_attn.w_vs.weight False\n",
      "encoder.layer_stack.2.slf_attn.w_vs.bias False\n",
      "encoder.layer_stack.2.slf_attn.layer_norm.weight False\n",
      "encoder.layer_stack.2.slf_attn.layer_norm.bias False\n",
      "encoder.layer_stack.2.slf_attn.fc.weight False\n",
      "encoder.layer_stack.2.slf_attn.fc.bias False\n",
      "encoder.layer_stack.2.pos_ffn.w_1.weight False\n",
      "encoder.layer_stack.2.pos_ffn.w_1.bias False\n",
      "encoder.layer_stack.2.pos_ffn.w_2.weight False\n",
      "encoder.layer_stack.2.pos_ffn.w_2.bias False\n",
      "encoder.layer_stack.2.pos_ffn.layer_norm.weight False\n",
      "encoder.layer_stack.2.pos_ffn.layer_norm.bias False\n",
      "encoder.layer_stack.3.slf_attn.w_qs.weight False\n",
      "encoder.layer_stack.3.slf_attn.w_qs.bias False\n",
      "encoder.layer_stack.3.slf_attn.w_ks.weight False\n",
      "encoder.layer_stack.3.slf_attn.w_ks.bias False\n",
      "encoder.layer_stack.3.slf_attn.w_vs.weight False\n",
      "encoder.layer_stack.3.slf_attn.w_vs.bias False\n",
      "encoder.layer_stack.3.slf_attn.layer_norm.weight False\n",
      "encoder.layer_stack.3.slf_attn.layer_norm.bias False\n",
      "encoder.layer_stack.3.slf_attn.fc.weight False\n",
      "encoder.layer_stack.3.slf_attn.fc.bias False\n",
      "encoder.layer_stack.3.pos_ffn.w_1.weight False\n",
      "encoder.layer_stack.3.pos_ffn.w_1.bias False\n",
      "encoder.layer_stack.3.pos_ffn.w_2.weight False\n",
      "encoder.layer_stack.3.pos_ffn.w_2.bias False\n",
      "encoder.layer_stack.3.pos_ffn.layer_norm.weight False\n",
      "encoder.layer_stack.3.pos_ffn.layer_norm.bias False\n",
      "encoder.layer_stack.4.slf_attn.w_qs.weight False\n",
      "encoder.layer_stack.4.slf_attn.w_qs.bias False\n",
      "encoder.layer_stack.4.slf_attn.w_ks.weight False\n",
      "encoder.layer_stack.4.slf_attn.w_ks.bias False\n",
      "encoder.layer_stack.4.slf_attn.w_vs.weight False\n",
      "encoder.layer_stack.4.slf_attn.w_vs.bias False\n",
      "encoder.layer_stack.4.slf_attn.layer_norm.weight False\n",
      "encoder.layer_stack.4.slf_attn.layer_norm.bias False\n",
      "encoder.layer_stack.4.slf_attn.fc.weight False\n",
      "encoder.layer_stack.4.slf_attn.fc.bias False\n",
      "encoder.layer_stack.4.pos_ffn.w_1.weight False\n",
      "encoder.layer_stack.4.pos_ffn.w_1.bias False\n",
      "encoder.layer_stack.4.pos_ffn.w_2.weight False\n",
      "encoder.layer_stack.4.pos_ffn.w_2.bias False\n",
      "encoder.layer_stack.4.pos_ffn.layer_norm.weight False\n",
      "encoder.layer_stack.4.pos_ffn.layer_norm.bias False\n",
      "encoder.layer_stack.5.slf_attn.w_qs.weight False\n",
      "encoder.layer_stack.5.slf_attn.w_qs.bias False\n",
      "encoder.layer_stack.5.slf_attn.w_ks.weight False\n",
      "encoder.layer_stack.5.slf_attn.w_ks.bias False\n",
      "encoder.layer_stack.5.slf_attn.w_vs.weight False\n",
      "encoder.layer_stack.5.slf_attn.w_vs.bias False\n",
      "encoder.layer_stack.5.slf_attn.layer_norm.weight False\n",
      "encoder.layer_stack.5.slf_attn.layer_norm.bias False\n",
      "encoder.layer_stack.5.slf_attn.fc.weight False\n",
      "encoder.layer_stack.5.slf_attn.fc.bias False\n",
      "encoder.layer_stack.5.pos_ffn.w_1.weight False\n",
      "encoder.layer_stack.5.pos_ffn.w_1.bias False\n",
      "encoder.layer_stack.5.pos_ffn.w_2.weight False\n",
      "encoder.layer_stack.5.pos_ffn.w_2.bias False\n",
      "encoder.layer_stack.5.pos_ffn.layer_norm.weight False\n",
      "encoder.layer_stack.5.pos_ffn.layer_norm.bias False\n",
      "variance_adaptor.pitch_bins False\n",
      "variance_adaptor.energy_bins False\n",
      "variance_adaptor.duration_predictor.conv_layer.conv1d_1.conv.weight False\n",
      "variance_adaptor.duration_predictor.conv_layer.conv1d_1.conv.bias False\n",
      "variance_adaptor.duration_predictor.conv_layer.layer_norm_1.weight False\n",
      "variance_adaptor.duration_predictor.conv_layer.layer_norm_1.bias False\n",
      "variance_adaptor.duration_predictor.conv_layer.conv1d_2.conv.weight False\n",
      "variance_adaptor.duration_predictor.conv_layer.conv1d_2.conv.bias False\n",
      "variance_adaptor.duration_predictor.conv_layer.layer_norm_2.weight False\n",
      "variance_adaptor.duration_predictor.conv_layer.layer_norm_2.bias False\n",
      "variance_adaptor.duration_predictor.linear_layer.weight False\n",
      "variance_adaptor.duration_predictor.linear_layer.bias False\n",
      "variance_adaptor.pitch_predictor.conv_layer.conv1d_1.conv.weight False\n",
      "variance_adaptor.pitch_predictor.conv_layer.conv1d_1.conv.bias False\n",
      "variance_adaptor.pitch_predictor.conv_layer.layer_norm_1.weight False\n",
      "variance_adaptor.pitch_predictor.conv_layer.layer_norm_1.bias False\n",
      "variance_adaptor.pitch_predictor.conv_layer.conv1d_2.conv.weight False\n",
      "variance_adaptor.pitch_predictor.conv_layer.conv1d_2.conv.bias False\n",
      "variance_adaptor.pitch_predictor.conv_layer.layer_norm_2.weight False\n",
      "variance_adaptor.pitch_predictor.conv_layer.layer_norm_2.bias False\n",
      "variance_adaptor.pitch_predictor.linear_layer.weight False\n",
      "variance_adaptor.pitch_predictor.linear_layer.bias False\n",
      "variance_adaptor.energy_predictor.conv_layer.conv1d_1.conv.weight False\n",
      "variance_adaptor.energy_predictor.conv_layer.conv1d_1.conv.bias False\n",
      "variance_adaptor.energy_predictor.conv_layer.layer_norm_1.weight False\n",
      "variance_adaptor.energy_predictor.conv_layer.layer_norm_1.bias False\n",
      "variance_adaptor.energy_predictor.conv_layer.conv1d_2.conv.weight False\n",
      "variance_adaptor.energy_predictor.conv_layer.conv1d_2.conv.bias False\n",
      "variance_adaptor.energy_predictor.conv_layer.layer_norm_2.weight False\n",
      "variance_adaptor.energy_predictor.conv_layer.layer_norm_2.bias False\n",
      "variance_adaptor.energy_predictor.linear_layer.weight False\n",
      "variance_adaptor.energy_predictor.linear_layer.bias False\n",
      "variance_adaptor.pitch_embedding.weight False\n",
      "variance_adaptor.energy_embedding.weight False\n",
      "decoder.position_enc False\n",
      "decoder.layer_stack.0.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.0.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.0.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.0.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.0.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.0.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.0.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.0.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.0.slf_attn.fc.weight False\n",
      "decoder.layer_stack.0.slf_attn.fc.bias False\n",
      "decoder.layer_stack.0.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.0.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.0.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.0.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.0.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.0.pos_ffn.layer_norm.bias False\n",
      "decoder.layer_stack.1.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.1.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.1.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.1.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.1.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.1.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.1.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.1.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.1.slf_attn.fc.weight False\n",
      "decoder.layer_stack.1.slf_attn.fc.bias False\n",
      "decoder.layer_stack.1.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.1.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.1.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.1.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.1.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.1.pos_ffn.layer_norm.bias False\n",
      "decoder.layer_stack.2.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.2.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.2.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.2.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.2.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.2.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.2.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.2.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.2.slf_attn.fc.weight False\n",
      "decoder.layer_stack.2.slf_attn.fc.bias False\n",
      "decoder.layer_stack.2.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.2.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.2.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.2.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.2.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.2.pos_ffn.layer_norm.bias False\n",
      "decoder.layer_stack.3.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.3.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.3.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.3.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.3.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.3.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.3.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.3.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.3.slf_attn.fc.weight False\n",
      "decoder.layer_stack.3.slf_attn.fc.bias False\n",
      "decoder.layer_stack.3.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.3.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.3.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.3.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.3.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.3.pos_ffn.layer_norm.bias False\n",
      "decoder.layer_stack.4.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.4.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.4.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.4.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.4.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.4.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.4.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.4.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.4.slf_attn.fc.weight False\n",
      "decoder.layer_stack.4.slf_attn.fc.bias False\n",
      "decoder.layer_stack.4.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.4.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.4.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.4.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.4.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.4.pos_ffn.layer_norm.bias False\n",
      "decoder.layer_stack.5.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.5.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.5.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.5.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.5.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.5.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.5.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.5.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.5.slf_attn.fc.weight False\n",
      "decoder.layer_stack.5.slf_attn.fc.bias False\n",
      "decoder.layer_stack.5.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.5.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.5.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.5.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.5.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.5.pos_ffn.layer_norm.bias False\n",
      "decoder.layer_stack.6.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.6.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.6.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.6.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.6.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.6.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.6.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.6.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.6.slf_attn.fc.weight False\n",
      "decoder.layer_stack.6.slf_attn.fc.bias False\n",
      "decoder.layer_stack.6.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.6.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.6.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.6.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.6.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.6.pos_ffn.layer_norm.bias False\n",
      "decoder.layer_stack.7.slf_attn.w_qs.weight False\n",
      "decoder.layer_stack.7.slf_attn.w_qs.bias False\n",
      "decoder.layer_stack.7.slf_attn.w_ks.weight False\n",
      "decoder.layer_stack.7.slf_attn.w_ks.bias False\n",
      "decoder.layer_stack.7.slf_attn.w_vs.weight False\n",
      "decoder.layer_stack.7.slf_attn.w_vs.bias False\n",
      "decoder.layer_stack.7.slf_attn.layer_norm.weight False\n",
      "decoder.layer_stack.7.slf_attn.layer_norm.bias False\n",
      "decoder.layer_stack.7.slf_attn.fc.weight False\n",
      "decoder.layer_stack.7.slf_attn.fc.bias False\n",
      "decoder.layer_stack.7.pos_ffn.w_1.weight False\n",
      "decoder.layer_stack.7.pos_ffn.w_1.bias False\n",
      "decoder.layer_stack.7.pos_ffn.w_2.weight False\n",
      "decoder.layer_stack.7.pos_ffn.w_2.bias False\n",
      "decoder.layer_stack.7.pos_ffn.layer_norm.weight False\n",
      "decoder.layer_stack.7.pos_ffn.layer_norm.bias False\n",
      "mel_linear.weight False\n",
      "mel_linear.bias False\n",
      "postnet.convolutions.0.0.conv.weight False\n",
      "postnet.convolutions.0.0.conv.bias False\n",
      "postnet.convolutions.0.1.weight False\n",
      "postnet.convolutions.0.1.bias False\n",
      "postnet.convolutions.1.0.conv.weight False\n",
      "postnet.convolutions.1.0.conv.bias False\n",
      "postnet.convolutions.1.1.weight False\n",
      "postnet.convolutions.1.1.bias False\n",
      "postnet.convolutions.2.0.conv.weight False\n",
      "postnet.convolutions.2.0.conv.bias False\n",
      "postnet.convolutions.2.1.weight False\n",
      "postnet.convolutions.2.1.bias False\n",
      "postnet.convolutions.3.0.conv.weight False\n",
      "postnet.convolutions.3.0.conv.bias False\n",
      "postnet.convolutions.3.1.weight False\n",
      "postnet.convolutions.3.1.bias False\n",
      "postnet.convolutions.4.0.conv.weight False\n",
      "postnet.convolutions.4.0.conv.bias False\n",
      "postnet.convolutions.4.1.weight False\n",
      "postnet.convolutions.4.1.bias False\n",
      "speaker_emb.weight True\n",
      "emotion_emb.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in fp.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87d060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
